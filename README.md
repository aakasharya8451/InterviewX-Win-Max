# InterviewX

**Offline AI-Powered Mock Interview Agent**

InterviewX is a completely offline mock interview platform that simulates realistic interview conversations using state-of-the-art AI models. Practice your interview skills privately without any data leaving your computer.

## âœ¨ Key Features

- ğŸ”’ **100% Offline** - All processing happens locally, no internet required after setup
- ğŸ¤– **AI-Powered Conversation** - Natural interview dialogue using Qwen3-4B (4 billion parameter model)
- ğŸ¤ **Real-Time Speech Recognition** - Advanced STT with Whisper and voice activity detection
- ğŸ”Š **Natural Voice Synthesis** - High-quality text-to-speech using Kokoro TTS
- ğŸ“Š **Performance Feedback** - AI-generated evaluation and rating after each session
- ğŸ’¼ **Professional Interface** - Clean, modern web UI with video conferencing experience
- ğŸ–¥ï¸ **Cross-Platform** - Runs on Windows, macOS, and Linux

## ğŸ“ Project Structure

```
InterviewX-Win-Max/
â”œâ”€â”€ Frontend/          # React + TypeScript web interface
â”‚   â”œâ”€â”€ src/          # React components, hooks, and types
â”‚   â””â”€â”€ README.md     # Frontend setup and development guide
â”œâ”€â”€ Backend/          # Python AI engine and API server
â”‚   â”œâ”€â”€ llm_engine.py    # Qwen3-4B conversation model
â”‚   â”œâ”€â”€ stt_engine.py    # Whisper speech-to-text
â”‚   â”œâ”€â”€ tts_engine.py    # Kokoro text-to-speech
â”‚   â”œâ”€â”€ backend_engine.py # FastAPI server
â”‚   â””â”€â”€ README.md     # Backend setup and model guide
â””â”€â”€ README.md         # This file
```

## ğŸ›  Technology Stack

### Frontend
- **Framework**: React 19 with TypeScript
- **Build Tool**: Vite
- **Styling**: TailwindCSS
- **Communication**: WebSocket for real-time audio streaming

### Backend
- **Language**: Python 3.9+
- **Web Framework**: FastAPI with WebSocket support
- **AI Models**:
  - **LLM**: Qwen3-4B (GGUF Q4_K_M, ~2.5 GB)
  - **STT**: OpenAI Whisper small (~466 MB)
  - **TTS**: Kokoro ONNX v1.0 (~60-120 MB)

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ and npm
- **Python** 3.9, 3.10, or 3.11
- **RAM**: 16 GB recommended for smooth LLM inference
- **Storage**: ~7 GB free space for AI models
- **Hardware**: Microphone and speakers/headphones

### Setup Steps

1. **Download AI Models**
   
   The backend requires several AI model files (~3-3.2 GB total). See detailed instructions in the [Backend README](Backend/README.md#required-model-files).

2. **Setup Backend**
   
   ```bash
   cd Backend
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # macOS/Linux
   source venv/bin/activate
   
   pip install fastapi uvicorn websockets llama-cpp-python pywhispercpp mediapipe webrtcvad pyaudio sounddevice soundfile numpy
   ```
   
   For detailed backend setup including model configuration, see [Backend README](Backend/README.md).

3. **Setup Frontend**
   
   ```bash
   cd Frontend
   npm install
   npm run build
   ```
   
   Then copy the build to Backend static folder:
   
   ```bash
   # Windows (from Backend directory)
   xcopy /E /I /Y ..\Frontend\dist static
   
   # macOS/Linux (from Backend directory)
   cp -r ../Frontend/dist/* static/
   ```
   
   For detailed frontend setup, see [Frontend README](Frontend/README.md).

4. **Run the Application**
   
   ```bash
   cd Backend
   python main.py
   ```
   
   The backend server starts on `http://localhost:8000` and automatically opens your browser to the interview interface.

## ğŸ“– Documentation

- **[Frontend Documentation](Frontend/README.md)** - React UI setup, development, and architecture
- **[Backend Documentation](Backend/README.md)** - AI engines, model setup, and API reference

## âš™ï¸ System Requirements

- **Operating System**: Windows 10+, macOS 10.15+, or Linux (Ubuntu 20.04+)
- **Memory**: Minimum 8 GB RAM, **16 GB recommended**
- **Storage**: ~7 GB for all AI models
- **GPU**: Optional (CPU-only operation fully supported)

## ğŸ”§ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Speaks â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Audio
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STT Engine      â”‚ Whisper + VAD
â”‚ (Speech-to-Text)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Text
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Engine      â”‚ Qwen3-4B generates
â”‚ (Interview AI)  â”‚ interview questions
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Response Text
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TTS Engine      â”‚ Kokoro converts
â”‚ (Text-to-Speech)â”‚ text to speech
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Audio
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Hears  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**End of Interview**: The LLM analyzes the entire conversation and provides spoken feedback plus a performance rating (1-3 scale).

## ğŸ¯ Usage

1. **Start Interview** - Click "Start Meeting" button
2. **Grant Permissions** - Allow microphone and camera access
3. **Begin Interview** - Say "start" or introduce yourself
4. **Answer Questions** - Respond naturally to AI interviewer's questions
5. **End Interview** - Click "End Call" when finished
6. **Get Feedback** - Receive AI-generated performance rating and feedback

## ğŸ“ License

This project uses various AI models and frameworks, each with their own licenses:
- **Qwen3**: Alibaba Cloud (Apache 2.0)
- **Whisper**: OpenAI (MIT)
- **Kokoro TTS**: Community contributors
- **FastAPI**: SebastiÃ¡n RamÃ­rez (MIT)

---

**Made with â¤ï¸ for offline AI-powered interview practice**
